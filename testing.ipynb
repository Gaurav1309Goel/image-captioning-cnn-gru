{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torchvision\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass Encoder(nn.Module):\n    def __init__(self, encoded_image_size=14):\n        super(Encoder, self).__init__()\n        self.enc_image_size = encoded_image_size\n        resnet = torchvision.models.mobilenet_v2(pretrained=True)\n        modules = list(resnet.children())[:-1]\n        self.resnet = nn.Sequential(*modules)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n    def forward(self, images):\n        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n        return out\n\n    def fine_tune(self, fine_tune=True):\n        for p in self.resnet.parameters():\n            p.requires_grad = False\n        for c in list(self.resnet.children())[5:]:\n            for p in c.parameters():\n                p.requires_grad = fine_tune","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"db = {}\nencoder.to(my_device)\nwith torch.no_grad():\n  for data in dataloader:\n      inputs, labels = data\n      inputs = inputs.to(my_device)\n      outputs = encoder(inputs)\n      for i,features in enumerate(outputs):\n        db[labels[i]] = features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torchvision\nimport random\n\nclass Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n\n    def forward(self, encoder_out, decoder_hidden):\n        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n\n        return attention_weighted_encoding, alpha\n\n\nclass DecoderWithAttention(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=1280, dropout=0.5,teacher_forcing_ratio = 1):\n        super(DecoderWithAttention, self).__init__()\n\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n        for p in self.embedding.parameters():\n            p.requires_grad = True\n        \n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()  # initialize some layers with the uniform distribution\n        self.teacher_forcing_ratio = teacher_forcing_ratio\n\n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n            \n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n\n        # Initialize LSTM state\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n\n        decode_lengths = (caption_lengths - 1).tolist()\n\n        # Create tensors to hold word predicion scores and alphas\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n\n\n        preds = embeddings[:,0,:]\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            teacher_force = random.random() < self.teacher_forcing_ratio\n            top1 = preds.argmax(1)\n            embeds_temp = self.embedding(top1)\n            next_input = embeddings[:batch_size_t, t, :] if teacher_force else embeds_temp[:batch_size_t,:]\n            h, c = self.decode_step(\n                torch.cat([next_input, attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n            \n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoder = DecoderWithAttention(512,512,512,len(vocab),1280)\ndecoder.to(device)\nmodel_dicts = torch.load('../input/flickr/decoder.pt')\ndecoder.load_state_dict(model_dicts)\ncriterion = nn.CrossEntropyLoss().to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load doc into memory\ndef load_doc(filename):\n    # open the file as read only\n    file = open(filename, 'r')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text\n \n# extract descriptions for images\ndef load_descriptions(doc):\n    mapping = dict()\n    # process lines\n    for line in doc.split('\\n'):\n        # split line by white space\n        tokens = line.split()\n        if len(line) < 2:\n            continue\n        # take the first token as the image id, the rest as the description\n        image_id, image_desc = tokens[0], tokens[1:]\n        # remove filename from image id\n        image_id = image_id.split('.')[0]\n        # convert description tokens back to string\n        image_desc = ' '.join(image_desc)\n        # store the first description for each image\n        if image_id not in mapping:\n            mapping[image_id] = image_desc\n    return mapping\n \nfilename = '../input/flickr/Flickr8K-20210412T103202Z-001/Flickr8K/Flickr8k_text/Flickr8k.token.txt'\ndoc = load_doc(filename)\ndescriptions = load_descriptions(doc)\nprint('Loaded: %d ' % len(descriptions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_all_descriptions(doc):\n    mapping = dict()\n    # process lines\n    for line in doc.split('\\n'):\n        # split line by white space\n        tokens = line.split()\n        if len(line) < 2:\n            continue\n        # take the first token as the image id, the rest as the description\n        image_id, image_desc = tokens[0], tokens[1:]\n        # remove filename from image id\n        image_id = image_id.split('.')[0]\n        # convert description tokens back to string\n        image_desc = ' '.join(image_desc)\n        # store the first description for each image\n        if image_id not in mapping:\n            mapping[image_id] = []\n        mapping[image_id].append(image_desc)\n    return mapping","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1 = '../input/flickr/Flickr8K-20210412T103202Z-001/Flickr8K/Flickr8k_text/Flickr_8k.trainImages.txt'\nf2 = '../input/flickr/Flickr8K-20210412T103202Z-001/Flickr8K/Flickr8k_text/Flickr_8k.valImages.txt'\ndoc = load_doc(f1)\ntraining = doc.split('\\n')\ndoc = load_doc(f2)\nvalidation = doc.split('\\n')\ndoc = load_doc(filename)\nall_desc = load_all_descriptions(doc)\nfrom torchtext.data.utils import get_tokenizer\nfrom collections import Counter\nfrom torchtext.vocab import Vocab\n\ntokenizer = get_tokenizer('basic_english')\ncounter = Counter()\nfor i in descriptions:\n    counter.update(tokenizer(descriptions[i]))\nstart_and_end = '<start> <end>'\ncounter.update(tokenizer(start_and_end))\nvocab = Vocab(counter, min_freq=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import sentence_bleu\n\n\ndef validate(val_loader, decoder, criterion):\n    \"\"\"\n    Performs one epoch's validation.\n    :param val_loader: DataLoader for validation data.\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param criterion: loss layer\n    :return: BLEU-4 score\n    \"\"\"\n    decoder.eval()  # eval mode (no dropout or batchnorm)\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top5accs = AverageMeter()\n    top1accs = AverageMeter()\n\n    start = time.time()\n\n    references = list()  # references (true captions) for calculating BLEU-4 score\n    hypotheses = list()  # hypotheses (predictions)\n\n    # explicitly disable gradient calculation to avoid CUDA memory error\n    # solves the issue #57\n    with torch.no_grad():\n        # Batches\n        for i, (encodings, caps, caplens, allcaps) in enumerate(val_loader):\n\n            # Move to device, if available\n            encodings = encodings.to(device)\n            caps = caps.to(device)\n            caplens = caplens.to(device)\n\n            # Forward prop.\n            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(encodings, caps, caplens)\n\n            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n            targets = caps_sorted[:, 1:]\n\n            # Remove timesteps that we didn't decode at, or are pads\n            # pack_padded_sequence is an easy trick to do this\n            scores_copy = scores.clone()\n            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n\n            # Calculate loss\n            loss = criterion(scores.data, targets.data)\n\n#             # Add doubly stochastic attention regularization\n            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n            # Keep track of metrics\n            losses.update(loss.item(), sum(decode_lengths))\n            top5 = accuracy(scores.data, targets.data, 5)\n            top5accs.update(top5, sum(decode_lengths))\n            top1 = accuracy(scores.data,targets.data,1)\n            top1accs.update(top1,sum(decode_lengths))\n            batch_time.update(time.time() - start)\n\n            start = time.time()\n\n            if i % print_freq == 0:\n                print('Validation: [{0}/{1}]\\t'\n                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n                                                                                loss=losses, top5=top5accs))\n            \n                print('Top-1 Accuracy {top1.val:.3f} ({top1.avg:.3f})'.format(top1=top1accs))\n\n\n            # Store references (true captions), and hypothesis (prediction) for each image\n            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n\n            # References\n            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n            for j in range(allcaps.shape[0]):\n                img_caps = allcaps[j].tolist()\n                img_captions = list(\n                    map(lambda c: [w for w in c if w not in {vocab['<start>'], vocab['<pad>']}],\n                        img_caps))  # remove <start> and pads\n                references.append(img_captions)\n\n            # Hypotheses\n            _, preds = torch.max(scores_copy, dim=2)\n            preds = preds.tolist()\n            temp_preds = list()\n            for j, p in enumerate(preds):\n                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n            preds = temp_preds\n            hypotheses.extend(preds)\n\n            assert len(references) == len(hypotheses)\n\n        # Calculate BLEU-4 scores\n        bleu4 = 0\n        cnt=0\n        for i in range(len(references)):\n            cnt+=1\n            bleu4+=sentence_bleu(references[i],hypotheses[i])\n        bleu4/=cnt\n        print('\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n                loss=losses,\n                top5=top5accs,\n                bleu=bleu4))\n\n    return bleu4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\npath = '../input/flickr/subjective_img/subjective_img/'\nfor y in os.listdir(path):\n    img_path = path + y\n    image = cv2.imread(img_path)\n    if image is not None:\n        image = cv2.resize(image, (227, 227))\n        data.append(image)\nfor i in range(len(data)):\n    data[i] = preprocess(np.uint8(data[i]))\n    \ndata = [t.numpy() for t in data]\ndata = torch.tensor(data, dtype = torch.float32)\ndata = data.to(my_device)\ndata1 = data\ndata1 = data1.to(torch.device('cpu'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch.nn.functional as F\n\ndef evaluate(loader,beam_size,word_map): \n    decoder.eval()\n    \n    hypotheses = list()\n    vocab_size = len(vocab)\n    # For each image\n    attentions = []\n    for i, (image) in enumerate(\n            tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n\n        k = beam_size        \n        image = image.unsqueeze(dim=0)\n\n        # Move to GPU device, if available\n        image = image.to(device)  # (1, 3, 256, 256)\n\n        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n        enc_image_size = encoder_out.size(1)\n        encoder_dim = encoder_out.size(3)\n\n        # Flatten encoding\n        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # We'll treat the problem as having a batch size of k\n        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n\n        # Tensor to store top k previous words at each step; now they're just <start>\n        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n\n        # Tensor to store top k sequences; now they're just <start>\n        seqs = k_prev_words  # (k, 1)\n\n        # Tensor to store top k sequences' scores; now they're just 0\n        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n\n        # Lists to store completed sequences and scores\n        complete_seqs = list()\n        complete_seqs_scores = list()\n\n        # Start decoding\n        complete_attention = []\n        attention_outputs = torch.zeros(k,51,num_pixels).to(device)\n        step = 1\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = decoder.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = decoder.init_c(mean_encoder_out)\n\n        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n        while True:\n#             attention_temp = torch.zeros(k,num_pixels)\n\n            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n\n            awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n\n            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n            awe = gate * awe\n\n            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n\n            scores = decoder.fc(h)  # (s, vocab_size)\n            scores = F.log_softmax(scores, dim=1)\n            # Add\n            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n\n            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n            if step == 1:\n                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n            else:\n                # Unroll and find top scores, and their unrolled indices\n                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n\n            # Convert unrolled indices to actual indices of scores\n            prev_word_inds = top_k_words // vocab_size  # (s)\n            next_word_inds = top_k_words % vocab_size  # (s)\n            next_word_inds1 = top_k_words\n\n            # Add new words to sequences\n            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n            attention_outputs = attention_outputs[prev_word_inds]\n            attention_outputs[:,step,:] = alpha[prev_word_inds]\n\n            # Which sequences are incomplete (didn't reach <end>)?\n            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n                               next_word != word_map['<end>']]\n            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n\n            # Set aside complete sequences\n            if len(complete_inds) > 0:\n                complete_seqs.extend(seqs[complete_inds].tolist())\n                complete_attention.extend(attention_outputs[complete_inds].tolist())\n                complete_seqs_scores.extend(top_k_scores[complete_inds])\n            k -= len(complete_inds)  # reduce beam length accordingly\n\n            # Proceed with incomplete sequences\n            if k == 0:\n                break\n            seqs = seqs[incomplete_inds]\n            attention_outputs = attention_outputs[incomplete_inds]\n            h = h[prev_word_inds[incomplete_inds]]\n            c = c[prev_word_inds[incomplete_inds]]\n            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n\n            # Break if things have been going on too long\n            if step > 50:\n                break\n            step += 1\n        i = complete_seqs_scores.index(max(complete_seqs_scores))\n        attentions.append(complete_attention[i])\n        seq = complete_seqs[i]\n        \n        hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n    return hypotheses,attentions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hypotheses,attentions = evaluate(data,1,vocab)\nattentions = torch.tensor(attentions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_attention(img, result, attention_plot):\n    #untransform\n    \n    img = img.transpose((1, 2, 0))\n    temp_image = img\n\n    fig = plt.figure(figsize=(15, 15))\n\n    len_result = len(result)\n    for l in range(len_result):\n        temp_att = attention_plot[l].reshape(7,-1)\n        \n        ax = fig.add_subplot(len_result//2,len_result//2, l+1)\n        ax.set_title(result[l])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.7, extent=img.get_extent())\n        \n\n    plt.tight_layout()\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_attention(data[2].to('cpu').numpy(),caps[0],attentions[2].unsqueeze(1))","metadata":{},"execution_count":null,"outputs":[]}]}